{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Недостатки MapReduce\n",
    "1) **Минимум 2 итерации записи на HDD** за job'у => не работает в реальном времени. Хотелось бы хранить сплиты в RAM.\n",
    "\n",
    "2) **Парадигма коротко живущих контейнеров.** Вспоминаем лекцию по YARN (приложения на сервис, приложения на задачу...). \n",
    "    * При запуске mapreduce-таски (например, маппера) YARN запустит контейнер\n",
    "    * контейнер умрёт его когда таска завершится. \n",
    "    * При аварии контейнер перезапустится на другой машине. \n",
    "Это удобно но старт-стоп контейнеров даёт Overhead если MapReduce-задач много.\n",
    "\n",
    "3) Нужно писать **очень много кода** (вспоминаем задачу на Join с позапрошлого семинара).\n",
    "\n",
    "4) По сути 1 источник данных - диск (HDFS, локальная ФС клиента... но всё равно диск). Хотелось бы уметь читать / писать в другие источники (базы данных, облачные хранилища).\n",
    "\n",
    "**Итог:** с MapReduce **можно** работать с BigData, но нельзя работать быстро."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **200Х годы**: нам нужна отказоустойчивая система. RAM на серверах мало. Будем сохранять промежуточные данные **на диск**.\n",
    "* **201Х годы**: \n",
    "    - Память становится дешевле и больше. \n",
    "    - Запросы от бизнеса на максимально быструю обработку (real-time).\n",
    " \n",
    "Диск использовать нецелесообразно - возвращаемся к RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Составляющие Spark-экосистемы\n",
    "\n",
    "Spark Написан на Scala, имеет Scala, Java, Python API.\n",
    "\n",
    "1. Spark Core - разбор кода, распределённое выполнение, поддержка отказоустойчивости.\n",
    "2. Аналог \"стандартной библиотеки\":\n",
    "   * Spark SQL - высокоуровненвая обработка с помощью pandas-подобного синтаксиса или SQL.\n",
    "   * Spark Streaming, Spark Structured Streaming - обработка (обновление результатов) данных в real-time\n",
    "   * MLLib - инструментарий для ML. Помимо Spark использует сторонние библиотеки (например Breeze, написанный на Fortran).\n",
    "3. Планировщики:\n",
    "   * Standalone - легковесный Spark на 1 машине. Использует встроенный планировщик\n",
    "   * Может использовать другие планировщики (например YARN, Mesos, Kubernetes).\n",
    "\n",
    "Подробнее **[здесь](https://www.oreilly.com/library/view/learning-spark/9781449359034/ch01.html)**.\n",
    "\n",
    "#### Источники данных\n",
    "![Image](images/datasources.png)\n",
    "\n",
    "В теории можем читать-писать в большое кол-во источников и приёмников данных.\n",
    "\n",
    "На практике:\n",
    "* Есть проблемы при взаимодействии с Hive (подробнее будет на лекции),\n",
    "* И при подключении к Cassandra.\n",
    "* Хорошо взаимодействует с Kafka."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Архитектура Spark-приложения\n",
    "![Image](images/cluster-overview.png)\n",
    "(https://spark.apache.org/docs/latest/cluster-overview.html)\n",
    "\n",
    "1. Driver program - управляющая программа.\n",
    "2. SparkContext - это основной объект, с помощью которого мы взаимодействуем со Spark.\n",
    "3. Cluster manager - планировщик (любой, см. выше).\n",
    "4. Executor - по сути JVM на нодах.\n",
    "\n",
    "В 1-м приближении работает также как и Hadoop. Единственное, контейнеры **долго живущие**. Контейнеры поднимаются 1 раз и умирают когда заканчивается SparkContext. Это позволяет хранить данные **в памяти JVM**. Быстрее RAM только кеши CPU, но это сложно реализуется (ассемблер)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Возможности работы со Spark\n",
    "##### Интерактивный shell\n",
    "\n",
    "1. `spark2-shell` - запускает Scala-оболочку.\n",
    "2. `pyspark2` - python оболочку.\n",
    "\n",
    "В этих оболочках уже имеется готовый SparkContext (переменная `sc`).\n",
    "\n",
    "##### Запуск файла на исполнение\n",
    "`spark2-submit [params] <file>` - можем запускать как jar-файлы, так и коды на Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Запуск Spark в Jupyter-ноутбуке:\n",
    "\n",
    "```bash\n",
    "PYSPARK_DRIVER_PYTHON=jupyter PYSPARK_PYTHON=/usr/bin/python3 PYSPARK_DRIVER_PYTHON_OPTS='notebook --ip=\"*\" --port=<PORT> --NotebookApp.token=\"<TOKEN>\" --no-browser' pyspark2 --master=yarn --num-executors=<N>\n",
    "```\n",
    " - **PORT** - порт, на котором откроется ноутбук.\n",
    " - **TOKEN** - токен, который нужно будет ввести для входа в Jupyter (любая строка). Не оставляйте токен пустым т.к. в этом случае к вашему ноутбуку смогут подключаться другие пользователи. `--NotebookApp.token=\"<TOKEN>\"` можно не писать, тогда он сгенерится сам, а посмотреть его можно будет с помощью команды `jupyter notebook list`.\n",
    " - **N** - кол-во executors (YARN containers), выделенных на приложение. \n",
    " \n",
    "Подробнее в [Userguide](https://docs.google.com/document/d/1dmb8o3M2ZCsjPq3rJQqd-jNLQhiBXWbWZcTn9aYUAp8/edit).\n",
    " \n",
    "#### Режимы запуска Spark\n",
    "1. **local**. И драйвер, и worker стартуют на 1 машине. Можно указывать число ядер, выделенных на задачу. Например, `local[3]`. Указывать меньше 2 не рекомендуется т.к. всегда запускает 2 процесса: driver, worker.\n",
    "2. **yarn**. Распределённый режим. Здесь можно дополнительно указать `--deploy-mode`. \n",
    "   * `cluster`. Драйвер на мастере либо на ноде. Рекомендуется для прода.\n",
    "   * `client`. Драйвер на клиенте. Проще отлаживаться. Проще работать в интерактивном режиме (сейчас мы работаем в режиме `client`). Но грузит клиент. \n",
    " \n",
    "В аргументах PySpark можно указывать и многе другое, подробнее [здесь](http://spark.apache.org/docs/latest/configuration.html#application-properties)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://mipt-client.atp-fivt.org:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0.cloudera2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можем изменить конфигурацию SparkContext, правда его придётся перезапустить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "conf = sc.getConf().setAppName(\"the {}\\'s spark app\".format(getpass.getuser())).set(\"spark.python.profile\",\"true\")\n",
    "sc.stop()\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.textFile(\"/data/griboedov\").map(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2681"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile(\"/data/griboedov\").map(lambda x: x.strip()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2681"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Profile of RDD<id=4>\n",
      "============================================================\n",
      "         29590 function calls (29584 primitive calls) in 0.028 seconds\n",
      "\n",
      "   Ordered by: internal time, cumulative time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "     5365    0.020    0.000    0.020    0.000 {method 'read' of '_io.BufferedReader' objects}\n",
      "     2684    0.002    0.000    0.024    0.000 serializers.py:677(loads)\n",
      "     2684    0.001    0.000    0.027    0.000 rdd.py:1055(<genexpr>)\n",
      "     2681    0.001    0.000    0.001    0.000 {method 'decode' of 'bytes' objects}\n",
      "     2684    0.001    0.000    0.002    0.000 serializers.py:714(read_int)\n",
      "     2684    0.001    0.000    0.025    0.000 serializers.py:686(load_stream)\n",
      "     2681    0.001    0.000    0.001    0.000 util.py:97(wrapper)\n",
      "     2681    0.000    0.000    0.001    0.000 <ipython-input-6-a3d81ddb872f>:1(<lambda>)\n",
      "     2684    0.000    0.000    0.000    0.000 {built-in method unpack}\n",
      "        6    0.000    0.000    0.027    0.005 {built-in method sum}\n",
      "        3    0.000    0.000    0.000    0.000 serializers.py:389(dump_stream)\n",
      "     2681    0.000    0.000    0.000    0.000 {method 'strip' of 'str' objects}\n",
      "        3    0.000    0.000    0.028    0.009 worker.py:370(process)\n",
      "      9/3    0.000    0.000    0.028    0.009 rdd.py:2498(pipeline_func)\n",
      "        3    0.000    0.000    0.000    0.000 serializers.py:721(write_int)\n",
      "        9    0.000    0.000    0.028    0.003 rdd.py:351(func)\n",
      "        3    0.000    0.000    0.027    0.009 rdd.py:1055(<lambda>)\n",
      "        3    0.000    0.000    0.000    0.000 serializers.py:575(dumps)\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method dumps}\n",
      "        3    0.000    0.000    0.000    0.000 rdd.py:323(func)\n",
      "        3    0.000    0.000    0.000    0.000 rdd.py:1046(<lambda>)\n",
      "        3    0.000    0.000    0.000    0.000 util.py:92(fail_on_stopiteration)\n",
      "        6    0.000    0.000    0.000    0.000 {method 'write' of '_io.BufferedWriter' objects}\n",
      "        6    0.000    0.000    0.000    0.000 rdd.py:909(func)\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method pack}\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method add}\n",
      "        6    0.000    0.000    0.000    0.000 {built-in method len}\n",
      "        3    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method iter}\n",
      "\n",
      "\n",
      "============================================================\n",
      "Profile of RDD<id=5>\n",
      "============================================================\n",
      "         29590 function calls (29584 primitive calls) in 0.048 seconds\n",
      "\n",
      "   Ordered by: internal time, cumulative time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "     5365    0.035    0.000    0.035    0.000 {method 'read' of '_io.BufferedReader' objects}\n",
      "     2684    0.003    0.000    0.042    0.000 serializers.py:677(loads)\n",
      "     2684    0.002    0.000    0.047    0.000 rdd.py:1055(<genexpr>)\n",
      "     2681    0.002    0.000    0.002    0.000 {method 'decode' of 'bytes' objects}\n",
      "     2684    0.002    0.000    0.003    0.000 serializers.py:714(read_int)\n",
      "     2681    0.001    0.000    0.002    0.000 util.py:97(wrapper)\n",
      "     2684    0.001    0.000    0.043    0.000 serializers.py:686(load_stream)\n",
      "     2681    0.001    0.000    0.001    0.000 <ipython-input-5-60f555e85c0e>:1(<lambda>)\n",
      "     2684    0.000    0.000    0.000    0.000 {built-in method unpack}\n",
      "        6    0.000    0.000    0.047    0.008 {built-in method sum}\n",
      "     2681    0.000    0.000    0.000    0.000 {method 'strip' of 'str' objects}\n",
      "        3    0.000    0.000    0.000    0.000 serializers.py:389(dump_stream)\n",
      "      9/3    0.000    0.000    0.048    0.016 rdd.py:2498(pipeline_func)\n",
      "        3    0.000    0.000    0.048    0.016 worker.py:370(process)\n",
      "        9    0.000    0.000    0.048    0.005 rdd.py:351(func)\n",
      "        3    0.000    0.000    0.000    0.000 serializers.py:721(write_int)\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method dumps}\n",
      "        3    0.000    0.000    0.048    0.016 rdd.py:1055(<lambda>)\n",
      "        3    0.000    0.000    0.000    0.000 serializers.py:575(dumps)\n",
      "        6    0.000    0.000    0.000    0.000 rdd.py:909(func)\n",
      "        3    0.000    0.000    0.000    0.000 rdd.py:323(func)\n",
      "        3    0.000    0.000    0.000    0.000 rdd.py:1046(<lambda>)\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method pack}\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method add}\n",
      "        6    0.000    0.000    0.000    0.000 {method 'write' of '_io.BufferedWriter' objects}\n",
      "        3    0.000    0.000    0.000    0.000 util.py:92(fail_on_stopiteration)\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method iter}\n",
      "        6    0.000    0.000    0.000    0.000 {built-in method len}\n",
      "        3    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sc.show_profiles()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resilient Distributed Dataset и ленивые вычисления\n",
    "\n",
    "RDD - набор данных, распределённый по партициям (аналог сплитов в Hadoop). Основной примитив работы в Spark. \n",
    "\n",
    "##### Свойства\n",
    "* Неизменяемый. Можем получить либо новый RDD, либо plain object\n",
    "* Итерируемый. Можем делать обход RDD\n",
    "* Восстанавливаемый. Каждая партиция помнит как она была получена (часть графа вычислений) и при утере может быть восстановлена."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создать RDD можно:\n",
    "* прочитав данные из источника\n",
    "* получить новый RDD из существующего."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====\t�������������� 1\r\n",
      "����������������\t��������������!.. ����! ������ ���������� �������� ������������!\r\n",
      "����������������\t���������� ������������������ ���������� - ����������,\r\n",
      "����������������\t\"�������� ����������\". - ���������� �������� ���� ��������,\r\n",
      "����������������\t���� ������, ���������������� ���� ������������������ ���� ����������.\r\n",
      "����������������\t������������ ������ ������������ ������ ��������������������,\r\n",
      "����������������\t���� ��������!.. �������������� ����...\r\n",
      "����������������\t��������������,\r\n",
      "����������������\t����! ���������� ����������������, ��������.\r\n",
      "����������������\t���������� ������������ �������� ���� ��������;\r\n",
      "cat: Unable to write to output stream.\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat /data/griboedov/gore_ot_uma-1.txt | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.textFile(\"/data/griboedov\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/data/griboedov MapPartitionsRDD[7] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идём в [SparkHistory UI](http://localhost:18089/) (для этого нужно пробросить порт 18089). Далее переходим в incompleted applications (приложение не завершилось т.к. SparkContext жив) и видим, что в списке Job пусто.\n",
    "\n",
    "Несмотря на это, сам RDD есть:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[8] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем кол-во объектов в RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2681"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Снова проверяем UI и... job'а появилась!\n",
    "\n",
    "В Spark'е сть 2 типа операций над RDD:\n",
    "* [трансформации](https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations). Преобразуют RDD в новое RDD.\n",
    "* [действия](https://spark.apache.org/docs/latest/rdd-programming-guide.html#actions). Преобразуют RDD в обычный объект.\n",
    "\n",
    "Трансформации выполяются **лениво**. При вызове трансформации достраивается граф вычислений и больше ничего не происходит. \n",
    "\n",
    "Реальное выполнение графа происходит при вызове Action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordCount на Spark\n",
    "\n",
    "Мы уже прочитали данные, теперь попробуем посчитать на них WordCount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# строим граф вычислений\n",
    "rdd = sc.textFile(\"/data/griboedov\")\n",
    "rdd = rdd.map(lambda x: x.strip().lower()) # приводим к нижнему регистру\n",
    "rdd = rdd.flatMap(lambda x: x.split(\" \")) # выделяем слова\n",
    "rdd = rdd.map(lambda x: (x, 1))  # собираем пары (word, 1)\n",
    "rdd = rdd.reduceByKey(lambda a, b: a + b) # суммируем \"1\" с одинаковыми ключами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = rdd.sortBy(lambda a: -a[1]) # сортируем по кол-ву встречаемости"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 432),\n",
       " ('в', 344),\n",
       " ('-', 296),\n",
       " ('и', 295),\n",
       " ('не', 287),\n",
       " ('я', 139),\n",
       " ('с', 129),\n",
       " ('на', 126),\n",
       " ('что', 104),\n",
       " ('*', 94)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(10) # Action!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Типы трансформаций в Spark\n",
    "\n",
    "![Image](images/stages.png)\n",
    "(https://www.slideshare.net/LisaHua/spark-overview-37479609)\n",
    "\n",
    "* Часть трансформаций (map, flatmap, ...) обрабатывает партиции независимо. Такие трансформации называются *narrow*. \n",
    "* reduce, sortBy аггрегируют данные и используют передачу по сети. Они называются *wide*. \n",
    "   * Wide-трансформации могут менять кол-во партиций.\n",
    "   * По wide-трансформациям происходит деление job'ы на Stages.\n",
    "\n",
    "Stage тоже делится на task'и. 1 task выполняется для одной партиции.\n",
    "\n",
    "**Итак: Task << Stage << Job << Application.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В Spark есть возможность вывести план job'ы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3) PythonRDD[58] at RDD at PythonRDD.scala:53 []\n",
      " |  MapPartitionsRDD[53] at mapPartitions at PythonRDD.scala:133 []\n",
      " |  ShuffledRDD[52] at partitionBy at NativeMethodAccessorImpl.java:0 []\n",
      " +-(3) PairwiseRDD[51] at sortBy at <ipython-input-20-d4e0638907ea>:1 []\n",
      "    |  PythonRDD[50] at sortBy at <ipython-input-20-d4e0638907ea>:1 []\n",
      "    |  MapPartitionsRDD[47] at mapPartitions at PythonRDD.scala:133 []\n",
      "    |  ShuffledRDD[46] at partitionBy at NativeMethodAccessorImpl.java:0 []\n",
      "    +-(3) PairwiseRDD[45] at reduceByKey at <ipython-input-19-83ffad06e866>:6 []\n",
      "       |  PythonRDD[44] at reduceByKey at <ipython-input-19-83ffad06e866>:6 []\n",
      "       |  /data/griboedov MapPartitionsRDD[43] at textFile at NativeMethodAccessorImpl.java:0 []\n",
      "       |  /data/griboedov HadoopRDD[42] at textFile at NativeMethodAccessorImpl.java:0 []\n"
     ]
    }
   ],
   "source": [
    "print(rdd.toDebugString().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим всего 3 трансформации. Где все остальные?\n",
    "\n",
    "Spark написан на Scala, которая под капотом использует JVM. Чтоб делать вычисления в Python, нужно вытаскивать данные из JVM. А потом возвращаться обратно. Получаем OverHead на сериализацию-десериализацию. Чтоб overhead'ов было меньше, схлопываем узкие трансформации в одну."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Весь пример целиком:** `/home/velkerr/seminars/pd2018/14-15-spark/griboedov.py`\n",
    "\n",
    "Запустим с помощью `spark2-submit griboedov.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 1.\n",
    "\n",
    "> При подсчёте отсеять пунктуацию и слова короче 3 символов. \n",
    "При фильтрации можно использовать регулярку: `re.sub(u\"\\\\W+\", \" \", x.strip(), flags=re.U)`.\n",
    "\n",
    "### Задача 2.\n",
    "\n",
    "> Считать только имена собственные. Именами собственными в данном случае будем считать такие слова, у которых 1-я буква заглавная, остальные - прописные.\n",
    "\n",
    "**Решение**: ` /home/velkerr/seminars/pd2018/14-15-spark/griboedov_adv.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Аккумуляторы\n",
    "\n",
    "Аналоги счётчиков в Hadoop. \n",
    "* Используется для легковесной аггрегации (без `reduceByKey` и дополнительных shuffle'ов)\n",
    "* Если аккумулятор используется в трансформациях, то нельзя гарантировать консистентность (мы можем с помощью action'a вызвать DAG несколько раз). Можно использовать в `foreach()`.\n",
    "\n",
    "**Объявление:** `cnt = sc.accumulator(start_val)`\n",
    "\n",
    "**Использование:** \n",
    "   * Inline: `foreach(lambda x: cnt.add(x))`\n",
    "   * Или же, с помощью своей функции:\n",
    "    ```python\n",
    "    def count_with_conditions(x):\n",
    "        global cnt\n",
    "        if ...:\n",
    "            cnt += 1\n",
    "\n",
    "    rdd.foreach(lambda x: count_with_conditions(x))\n",
    "    ```\n",
    "\n",
    "**Получение результата:** `cnt.value`\n",
    "\n",
    "Подробнее в [документации](http://spark.apache.org/docs/latest/rdd-programming-guide.html#accumulators)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 3.\n",
    "\n",
    "> Переделайте задача 2 так, чтоб кол-во имён собственных вычислялось с помощью аккумулятора.\n",
    "\n",
    "**Решение**: ` /home/velkerr/seminars/pd2018/14-15-spark/griboedov_accum.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcast-переменные\n",
    "\n",
    "Аналог DistributedCache в Hadoop. Обычно используется когда мы хотим в спарке сделать Map-side join (т.е. имеется 2 датасета: 1 маленький, который и добавляем в broadcast, другой большой)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "br_cast = sc.broadcast([\"hadoop\", \"hive\", \"spark\", 'zookeeper', 'kafka']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "br_cast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "br_cast.value[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Кеширование\n",
    "\n",
    "При перезапуске Action, пересчитывается весь граф вычислений. Это логично т.к. в трансформациях ничего не вычисляется. Полезно это тем, что если за время работы задачи данные обновились (дополнились), нам достаточно просто перевызвать Action.\n",
    "\n",
    "Но если данные не меняются (например, при отладке), такой пересчёт даёт Overhead. Можно **закешировать** часть pipeline. Тогда при след. вызове Action, RDD считается с кеша и пересчёт начнётся с того места, где было кеширование. В History UI все Stage перед этим будут помечены \"Skipped\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.textFile(\"/data/griboedov\")\n",
    "rdd = rdd.map(lambda x: x.strip().lower())\n",
    "rdd = rdd.flatMap(lambda x: x.split(\" \"))\n",
    "rdd = rdd.map(lambda x: (x, 1)).cache() # тут всё хорошо работает, кешируем\n",
    "rdd = rdd.reduceByKey(lambda a, b: a + b) # а тут хотим отладить, поэтому будут перезапуски\n",
    "rdd = rdd.sortBy(lambda a: -a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 432), ('в', 344), ('-', 296), ('и', 295), ('не', 287)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[556] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кешировать можно с помощью двух операций:\n",
    "* `cache()`\n",
    "* `persist(storage_level)`\n",
    "\n",
    "В `persist()` можно указать [StorageLevel](https://spark.apache.org/docs/2.1.2/api/python/_modules/pyspark/storagelevel.html), т.е. на какой носитель кешируем. Можем закешировать в диск, в память, на диск и / или память на несколько нод... или дать возможность Spark'у решить самому (на основе объёма кеша).\n",
    "\n",
    "`cache()` - это простой вариант `persist()`, когда кешируем только в RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.textFile(\"/data/griboedov\")\n",
    "rdd = rdd.map(lambda x: x.strip().lower())\n",
    "rdd = rdd.flatMap(lambda x: x.split(\" \"))\n",
    "rdd = rdd.map(lambda x: (x, 1)).cache() # тут всё хорошо работает, кешируем\n",
    "rdd = rdd.reduceByKey(lambda a, b: a + b) # а тут хотим отладить, поэтому будут перезапуски"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = rdd.sortBy(lambda a: -a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[97] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 432), ('в', 344), ('-', 296), ('и', 295), ('не', 287)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Практические задания"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В hdfs в папке `/data/access_logs/big_log` лежит лог в формате\n",
    "\n",
    "* IP-адрес пользователя (`195.206.123.39`),\n",
    "* Далее идут два неиспользуемых в нашем случае поля (`-` и `-`),\n",
    "* Время запроса (`[24/Sep/2015:12:32:53 +0400]`),\n",
    "* Строка запроса (`\"GET /id18222 HTTP/1.1\"`),\n",
    "* HTTP-код ответа (`200`),\n",
    "* Размер ответа (`10703`),\n",
    "* Реферер (источник перехода; `\"http://bing.com/\"`),\n",
    "* Идентификационная строка браузера (User-Agent; `\"Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/37.0.2062.94 Safari/537.36\"`).\n",
    "\n",
    "Созданы несколько семплов данных разного размера:\n",
    "```\n",
    "3.4 G    10.2 G   /data/access_logs/big_log\n",
    "17.6 M   52.7 M   /data/access_logs/big_log_10000\n",
    "175.4 M  526.2 M  /data/access_logs/big_log_100000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример парсинга логов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATASET = \"/data/access_logs/big_log_10000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime as dt\n",
    "\n",
    "log_format = re.compile( \n",
    "    r\"(?P<host>[\\d\\.]+)\\s\" \n",
    "    r\"(?P<identity>\\S*)\\s\" \n",
    "    r\"(?P<user>\\S*)\\s\"\n",
    "    r\"\\[(?P<time>.*?)\\]\\s\"\n",
    "    r'\"(?P<request>.*?)\"\\s'\n",
    "    r\"(?P<status>\\d+)\\s\"\n",
    "    r\"(?P<bytes>\\S*)\\s\"\n",
    "    r'\"(?P<referer>.*?)\"\\s'\n",
    "    r'\"(?P<user_agent>.*?)\"\\s*'\n",
    ")\n",
    "\n",
    "def parseLine(line):\n",
    "    match = log_format.match(line)\n",
    "    if not match:\n",
    "        return (\"\", \"\", \"\", \"\", \"\", \"\", \"\" ,\"\", \"\")\n",
    "\n",
    "    request = match.group('request').split()\n",
    "    return (match.group('host'), match.group('time').split()[0], \\\n",
    "       request[0], request[1], match.group('status'), match.group('bytes'), \\\n",
    "        match.group('referer'), match.group('user_agent'),\n",
    "        dt.strptime(match.group('time').split()[0], '%d/%b/%Y:%H:%M:%S').hour)\n",
    "\n",
    "\n",
    "lines = sc.textFile(DATASET)\n",
    "parsed_logs = lines.map(parseLine).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('109.105.128.100',\n",
       "  '10/Dec/2015:00:00:00',\n",
       "  'GET',\n",
       "  '/id45574',\n",
       "  '200',\n",
       "  '27513',\n",
       "  '-',\n",
       "  'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.99 Safari/537.36',\n",
       "  0),\n",
       " ('217.146.45.122',\n",
       "  '10/Dec/2015:00:00:00',\n",
       "  'GET',\n",
       "  '/id40851',\n",
       "  '200',\n",
       "  '11914',\n",
       "  '-',\n",
       "  'Mozilla/5.0 (X11; Linux i686; rv:10.0.4) Gecko/20120421 Firefox/10.0.4',\n",
       "  0),\n",
       " ('17.72.78.198',\n",
       "  '10/Dec/2015:00:00:00',\n",
       "  'GET',\n",
       "  '/id58931',\n",
       "  '200',\n",
       "  '32457',\n",
       "  '-',\n",
       "  'Mozilla/5.0; TOB 6.11 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko',\n",
       "  0),\n",
       " ('46.245.183.68',\n",
       "  '10/Dec/2015:00:00:00',\n",
       "  'GET',\n",
       "  '/id19513',\n",
       "  '200',\n",
       "  '26190',\n",
       "  '-',\n",
       "  'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.99 Safari/537.36',\n",
       "  0),\n",
       " ('91.197.164.156',\n",
       "  '10/Dec/2015:00:00:01',\n",
       "  'GET',\n",
       "  '/id39028',\n",
       "  '200',\n",
       "  '14115',\n",
       "  '-',\n",
       "  'Mozilla/5.0 (X11; Fedora; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.125 Safari/537.36',\n",
       "  0),\n",
       " ('93.92.113.28',\n",
       "  '10/Dec/2015:00:00:01',\n",
       "  'GET',\n",
       "  '/id62078',\n",
       "  '200',\n",
       "  '18177',\n",
       "  '-',\n",
       "  'Mozilla/5.0 (iPad; CPU OS 9_0 like Mac OS X) AppleWebKit/600.1.4 (KHTML, like Gecko) CriOS/45.0.2454.89 Mobile/13A344 Safari/600.1.4',\n",
       "  0),\n",
       " ('91.238.133.210',\n",
       "  '10/Dec/2015:00:00:01',\n",
       "  'GET',\n",
       "  '/id92791',\n",
       "  '200',\n",
       "  '19835',\n",
       "  '-',\n",
       "  'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.99 Safari/537.36',\n",
       "  0),\n",
       " ('93.92.113.28',\n",
       "  '10/Dec/2015:00:00:02',\n",
       "  'GET',\n",
       "  '/id67125',\n",
       "  '200',\n",
       "  '23795',\n",
       "  '-',\n",
       "  'Mozilla/5.0 (iPad; CPU OS 9_0 like Mac OS X) AppleWebKit/600.1.4 (KHTML, like Gecko) CriOS/45.0.2454.89 Mobile/13A344 Safari/600.1.4',\n",
       "  0),\n",
       " ('92.227.170.8',\n",
       "  '10/Dec/2015:00:00:02',\n",
       "  'GET',\n",
       "  '/id43630',\n",
       "  '200',\n",
       "  '9841',\n",
       "  '-',\n",
       "  'Mozilla/5.0 (Linux; Android 5.0.1; SCH-I545 Build/LRX22C) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/43.0.2357.93 Mobile Safari/537.36',\n",
       "  0),\n",
       " ('185.36.12.188',\n",
       "  '10/Dec/2015:00:00:03',\n",
       "  'GET',\n",
       "  '/id34798',\n",
       "  '200',\n",
       "  '15053',\n",
       "  '-',\n",
       "  'Mozilla/5.0 (Windows NT 5.1; U; de; rv:1.9.1.6) Gecko/20091201 Firefox/3.5.6 Opera 11.00',\n",
       "  0)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_logs.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2й вариант парсинга - с помощью namedtuple\n",
    "\n",
    "```python\n",
    "LogItem = namedtuple(\"LogItem\", \n",
    "                     [\"host\", \"time\", \"method\", \"path\", \"status\", \"length\", \"referer\", \"user_agent\", \"hour\"])\n",
    "\n",
    "def parseLine(line):\n",
    "    match = log_format.match(line)\n",
    "    if not match:\n",
    "        return LogItem(\"\", \"\", \"\", \"\", \"\", \"\", \"\" ,\"\", \"\")\n",
    "\n",
    "    request = match.group('request').split()\n",
    "    return LogItem(\n",
    "        host=match.group('host'),\n",
    "        time=match.group('time').split()[0],\n",
    "        method=request[0],\n",
    "        path=request[1],\n",
    "        status=match.group('status'),\n",
    "        length=match.group('bytes'),\n",
    "        referer=match.group('referer'),\n",
    "        user_agent=match.group('user_agent'),\n",
    "        hour=dt.strptime(match.group('time').split()[0],'%d/%b/%Y:%H:%M:%S').hour\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Распарсили, получили RDD, закешировали."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('109.105.128.100',\n",
       "  '10/Dec/2015:00:00:00',\n",
       "  'GET',\n",
       "  '/id45574',\n",
       "  '200',\n",
       "  '27513',\n",
       "  '-',\n",
       "  'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.99 Safari/537.36',\n",
       "  0)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_logs.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "from pyspark.sql import SparkSession, Row\n",
    "\n",
    "log_format = re.compile(\n",
    "    r\"(?P<host>[\\d\\.]+)\\s\"\n",
    "    r\"(?P<identity>\\S*)\\s\"\n",
    "    r\"(?P<user>\\S*)\\s\"\n",
    "    r\"\\[(?P<time>.*?)\\]\\s\"\n",
    "    r'\"(?P<request>.*?)\"\\s'\n",
    "    r\"(?P<status>\\d+)\\s\"\n",
    "    r\"(?P<bytes>\\S*)\\s\"\n",
    "    r'\"(?P<referer>.*?)\"\\s'\n",
    "    r'\"(?P<user_agent>.*?)\"\\s*'\n",
    ")\n",
    "\n",
    "\n",
    "def parseLine(line):\n",
    "    match = log_format.match(line)\n",
    "    if not match:\n",
    "        return (\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\")\n",
    "\n",
    "    request = match.group('request').split()\n",
    "    return (match.group('host'), match.group('time').split()[0],\n",
    "        request[0], request[1], match.group('status'), int(match.group('bytes')),\n",
    "        match.group('referer'), match.group('user_agent'),\n",
    "        dt.strptime(match.group('time').split()[0], '%d/%b/%Y:%H:%M:%S').hour)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spark_session = SparkSession.builder.master(\"yarn\").appName(\"501 df\").config(\"spark.ui.port\", \"18089\").getOrCreate()\n",
    "    lines = spark_session.sparkContext.textFile(\"%s\" % sys.argv[1])\n",
    "    parts = lines.map(parseLine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 4.\n",
    "> Напишите программу, выводящую на экран TOP5 ip адресов, в которых содержится хотя бы одна цифра 4, с наибольшим количеством посещений.\n",
    "Каждая строка результата должна содержать IP адрес и число посещений, разделенные табуляцией, строки должны быть упорядочены по числу посещений по убыванию, например:\n",
    "```\n",
    "195.206.123.39<TAB>40\n",
    "196.206.123.40<TAB>39\n",
    "191.206.123.41<TAB>38\n",
    "175.206.123.42<TAB>37\n",
    "195.236.123.43<TAB>36\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[82] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 5.\n",
    ">  Напишите программу, выводящую на экран суммарное распределение количества посетителей по часам (для каждого часа в сутках вывести количество посетителей, пришедших в этот час). Id посетителя = ip + user_agent.\n",
    "Результат должен содержать час в сутках и число посетителей, разделенные табом и упорядоченные по часам. Например:\n",
    "```\n",
    "0<tab>10\n",
    "1<tab>10\n",
    "2<tab>10\n",
    "…..\n",
    "21<tab>30\n",
    "22<tab>20\n",
    "23<tab>10\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[82] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.format('csv').option('sep', ' ').load('/data/access_logs/big_log_10000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd = df.limit(1).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv('/data/twitter/twitter_sample_small.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_c0</th>\n",
       "      <th>_c1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>2241</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  _c0   _c1\n",
       "0  12  2241"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.limit(1).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+\n",
      "|destination|source|\n",
      "+-----------+------+\n",
      "|         12|  2241|\n",
      "+-----------+------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    F.col(\"_c0\").alias(\"destination\"),\n",
    "    F.col(\"_c1\").alias(\"source\")\n",
    ").show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1 = df.select(\n",
    "    F.col(\"_c0\").alias(\"destination\"),\n",
    "    F.col(\"_c1\").alias(\"source\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>destination</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>2241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>13349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>41873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>82473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>414853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>755452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>12</td>\n",
       "      <td>758983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>12</td>\n",
       "      <td>793023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>12</td>\n",
       "      <td>794748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12</td>\n",
       "      <td>806280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12</td>\n",
       "      <td>873731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>884271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>959551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12</td>\n",
       "      <td>1016301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>12</td>\n",
       "      <td>1070441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>12</td>\n",
       "      <td>1190051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>12</td>\n",
       "      <td>1190411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>12</td>\n",
       "      <td>1467871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>12</td>\n",
       "      <td>2039131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>12</td>\n",
       "      <td>2049521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>12</td>\n",
       "      <td>2683161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>12</td>\n",
       "      <td>3038461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>12</td>\n",
       "      <td>3140001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>12</td>\n",
       "      <td>3171031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>12</td>\n",
       "      <td>3259651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>12</td>\n",
       "      <td>3492361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>12</td>\n",
       "      <td>3640861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>12</td>\n",
       "      <td>3856841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>12</td>\n",
       "      <td>4276531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>12</td>\n",
       "      <td>4566851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54455</th>\n",
       "      <td>688</td>\n",
       "      <td>16828867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54456</th>\n",
       "      <td>689</td>\n",
       "      <td>47515341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54457</th>\n",
       "      <td>690</td>\n",
       "      <td>32808589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54458</th>\n",
       "      <td>699</td>\n",
       "      <td>35006947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54459</th>\n",
       "      <td>711</td>\n",
       "      <td>608673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54460</th>\n",
       "      <td>711</td>\n",
       "      <td>6078972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54461</th>\n",
       "      <td>711</td>\n",
       "      <td>6475512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54462</th>\n",
       "      <td>711</td>\n",
       "      <td>7163912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54463</th>\n",
       "      <td>711</td>\n",
       "      <td>7609922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54464</th>\n",
       "      <td>711</td>\n",
       "      <td>7962562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54465</th>\n",
       "      <td>711</td>\n",
       "      <td>12469782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54466</th>\n",
       "      <td>711</td>\n",
       "      <td>15678244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54467</th>\n",
       "      <td>714</td>\n",
       "      <td>5462572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54468</th>\n",
       "      <td>714</td>\n",
       "      <td>14822373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54469</th>\n",
       "      <td>717</td>\n",
       "      <td>14977411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54470</th>\n",
       "      <td>717</td>\n",
       "      <td>38686706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54471</th>\n",
       "      <td>718</td>\n",
       "      <td>1159251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54472</th>\n",
       "      <td>718</td>\n",
       "      <td>1489411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54473</th>\n",
       "      <td>718</td>\n",
       "      <td>6453802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54474</th>\n",
       "      <td>718</td>\n",
       "      <td>6630042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54475</th>\n",
       "      <td>718</td>\n",
       "      <td>14604664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54476</th>\n",
       "      <td>718</td>\n",
       "      <td>14932117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54477</th>\n",
       "      <td>718</td>\n",
       "      <td>19784748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54478</th>\n",
       "      <td>718</td>\n",
       "      <td>26249386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54479</th>\n",
       "      <td>719</td>\n",
       "      <td>10813132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54480</th>\n",
       "      <td>721</td>\n",
       "      <td>15220240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54481</th>\n",
       "      <td>721</td>\n",
       "      <td>15983337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54482</th>\n",
       "      <td>722</td>\n",
       "      <td>4448101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54483</th>\n",
       "      <td>722</td>\n",
       "      <td>5784082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54484</th>\n",
       "      <td>722</td>\n",
       "      <td>14586588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>54485 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      destination    source\n",
       "0              12      2241\n",
       "1              12     13349\n",
       "2              12     41873\n",
       "3              12     82473\n",
       "4              12    414853\n",
       "5              12    755452\n",
       "6              12    758983\n",
       "7              12    793023\n",
       "8              12    794748\n",
       "9              12    806280\n",
       "10             12    873731\n",
       "11             12    884271\n",
       "12             12    959551\n",
       "13             12   1016301\n",
       "14             12   1070441\n",
       "15             12   1190051\n",
       "16             12   1190411\n",
       "17             12   1467871\n",
       "18             12   2039131\n",
       "19             12   2049521\n",
       "20             12   2683161\n",
       "21             12   3038461\n",
       "22             12   3140001\n",
       "23             12   3171031\n",
       "24             12   3259651\n",
       "25             12   3492361\n",
       "26             12   3640861\n",
       "27             12   3856841\n",
       "28             12   4276531\n",
       "29             12   4566851\n",
       "...           ...       ...\n",
       "54455         688  16828867\n",
       "54456         689  47515341\n",
       "54457         690  32808589\n",
       "54458         699  35006947\n",
       "54459         711    608673\n",
       "54460         711   6078972\n",
       "54461         711   6475512\n",
       "54462         711   7163912\n",
       "54463         711   7609922\n",
       "54464         711   7962562\n",
       "54465         711  12469782\n",
       "54466         711  15678244\n",
       "54467         714   5462572\n",
       "54468         714  14822373\n",
       "54469         717  14977411\n",
       "54470         717  38686706\n",
       "54471         718   1159251\n",
       "54472         718   1489411\n",
       "54473         718   6453802\n",
       "54474         718   6630042\n",
       "54475         718  14604664\n",
       "54476         718  14932117\n",
       "54477         718  19784748\n",
       "54478         718  26249386\n",
       "54479         719  10813132\n",
       "54480         721  15220240\n",
       "54481         721  15983337\n",
       "54482         722   4448101\n",
       "54483         722   5784082\n",
       "54484         722  14586588\n",
       "\n",
       "[54485 rows x 2 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|  source|count|\n",
      "+--------+-----+\n",
      "|19593065|    1|\n",
      "|20651832|    1|\n",
      "|21215059|    1|\n",
      "|21240863|    1|\n",
      "|21705463|    1|\n",
      "|22197844|    1|\n",
      "|22377590|    1|\n",
      "|22935113|    1|\n",
      "|23993819|    1|\n",
      "|24268091|    1|\n",
      "|24299946|    1|\n",
      "|24326074|    1|\n",
      "|25053014|    1|\n",
      "|26715269|    1|\n",
      "|27121291|    1|\n",
      "|27385940|    1|\n",
      "|27628353|    1|\n",
      "|28817312|    1|\n",
      "|29847995|    1|\n",
      "|30290044|    1|\n",
      "+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('source').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|  source|count|\n",
      "+--------+-----+\n",
      "|      53|    7|\n",
      "| 9598762|    4|\n",
      "|15458708|    4|\n",
      "|13342022|    4|\n",
      "|      20|    4|\n",
      "|19489341|    4|\n",
      "|      12|    4|\n",
      "|14206015|    3|\n",
      "|26468557|    3|\n",
      "|14287820|    3|\n",
      "|     107|    3|\n",
      "|52041136|    3|\n",
      "|17184081|    3|\n",
      "|19788155|    3|\n",
      "|21494147|    3|\n",
      "|18662758|    3|\n",
      "|18234522|    3|\n",
      "|16227030|    3|\n",
      "|47516482|    3|\n",
      "|      23|    3|\n",
      "+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('source').count().orderBy(F.col('count').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum,avg,max,count\n",
    "\n",
    "df1 = df1.groupBy(\"source\").agg(\n",
    "    count(\"*\").alias(\"count1\"), \n",
    "    avg(\"source\").alias(\"mean\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[source: string, count1: bigint, mean: double]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+\n",
      "|  source|count1|       mean|\n",
      "+--------+------+-----------+\n",
      "|19593065|     1|1.9593065E7|\n",
      "|20651832|     1|2.0651832E7|\n",
      "|21215059|     1|2.1215059E7|\n",
      "|21240863|     1|2.1240863E7|\n",
      "|21705463|     1|2.1705463E7|\n",
      "|22197844|     1|2.2197844E7|\n",
      "|22377590|     1| 2.237759E7|\n",
      "|22935113|     1|2.2935113E7|\n",
      "|23993819|     1|2.3993819E7|\n",
      "|24268091|     1|2.4268091E7|\n",
      "|24299946|     1|2.4299946E7|\n",
      "|24326074|     1|2.4326074E7|\n",
      "|25053014|     1|2.5053014E7|\n",
      "|26715269|     1|2.6715269E7|\n",
      "|27121291|     1|2.7121291E7|\n",
      "|27385940|     1| 2.738594E7|\n",
      "|27628353|     1|2.7628353E7|\n",
      "|28817312|     1|2.8817312E7|\n",
      "|29847995|     1|2.9847995E7|\n",
      "|30290044|     1|3.0290044E7|\n",
      "+--------+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12,422,53,52,107,20,23,274,34\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "TweeterDF = spark.read.format(\"csv\")\\\n",
    "          .option(\"sep\", \"\\t\")\\\n",
    "          .load(\"/data/twitter/twitter_sample_small.txt\")\n",
    "\n",
    "PeakDF = TweeterDF\\\n",
    "    .select(\n",
    "        F.col(\"_c0\").alias(\"destination\"),\n",
    "        F.col(\"_c1\").alias(\"source\"))\\\n",
    "    .groupBy(\"source\")\\\n",
    "    .agg(F.collect_list(F.col(\"destination\")).alias(\"all_peak\"))\\\n",
    "    .persist()\n",
    "\n",
    "NewStageDF = PeakDF\\\n",
    "    .filter(F.col(\"source\") == F.lit(\"12\"))\\\n",
    "    .select(\n",
    "        F.col(\"source\").alias(\"path\"),\n",
    "        F.col(\"all_peak\").alias(\"last_peak\"))\n",
    "    \n",
    "NewStageDF = broadcast(NewStageDF)\n",
    "\n",
    "while not NewStageDF.select(F.expr(\"array_contains(last_peak, '34')\")).first()[0]:\n",
    "    NewStageDF = NewStageDF\\\n",
    "        .alias(\"init\")\\\n",
    "        .join(\n",
    "            PeakDF.alias(\"df\"),\n",
    "            F.expr(\"array_contains(init.last_peak, df.source)\"))\\\n",
    "        .distinct()\\\n",
    "        .select(\n",
    "            F.concat_ws(\",\", F.col(\"init.path\"), F.col(\"df.source\")).alias(\"path\"),\n",
    "            F.col(\"df.all_peak\").alias(\"last_peak\"))\n",
    "\n",
    "path = NewStageDF\\\n",
    "    .filter(F.expr(\"array_contains(last_peak, '34')\"))\\\n",
    "    .select(F.concat_ws(\",\", F.col(\"path\"), F.lit(\"34\")).alias(\"path\"))\\\n",
    "    .collect()[0][0]\n",
    "\n",
    "PeakDF.unpersist()\n",
    "\n",
    "print(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
