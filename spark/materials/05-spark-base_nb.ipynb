{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Недостатки MapReduce\n",
    "1) **Минимум 2 итерации записи на HDD** за job'у => не работает в реальном времени. Хотелось бы хранить сплиты в RAM.\n",
    "\n",
    "2) **Парадигма коротко живущих контейнеров.** Вспоминаем лекцию по YARN (приложения на сервис, приложения на задачу...). \n",
    "    * При запуске mapreduce-таски (например, маппера) YARN запустит контейнер\n",
    "    * контейнер умрёт его когда таска завершится. \n",
    "    * При аварии контейнер перезапустится на другой машине. \n",
    "Это удобно но старт-стоп контейнеров даёт Overhead если MapReduce-задач много.\n",
    "\n",
    "3) Нужно писать **очень много кода** (вспоминаем задачу на Join с позапрошлого семинара).\n",
    "\n",
    "4) По сути 1 источник данных - диск (HDFS, локальная ФС клиента... но всё равно диск). Хотелось бы уметь читать / писать в другие источники (базы данных, облачные хранилища).\n",
    "\n",
    "**Итог:** с MapReduce **можно** работать с BigData, но нельзя работать быстро."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **200Х годы**: нам нужна отказоустойчивая система. RAM на серверах мало. Будем сохранять промежуточные данные **на диск**.\n",
    "* **201Х годы**: \n",
    "    - Память становится дешевле и больше. \n",
    "    - Запросы от бизнеса на максимально быструю обработку (real-time).\n",
    " \n",
    "Диск использовать нецелесообразно - возвращаемся к RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Составляющие Spark-экосистемы\n",
    "\n",
    "Spark Написан на Scala, имеет Scala, Java, Python API.\n",
    "\n",
    "1. Spark Core - разбор кода, распределённое выполнение, поддержка отказоустойчивости.\n",
    "2. Аналог \"стандартной библиотеки\":\n",
    "   * Spark SQL - высокоуровненвая обработка с помощью pandas-подобного синтаксиса или SQL.\n",
    "   * Spark Streaming, Spark Structured Streaming - обработка (обновление результатов) данных в real-time\n",
    "   * MLLib - инструментарий для ML. Помимо Spark использует сторонние библиотеки (например Breeze, написанный на Fortran).\n",
    "3. Планировщики:\n",
    "   * Standalone - легковесный Spark на 1 машине. Использует встроенный планировщик\n",
    "   * Может использовать другие планировщики (например YARN, Mesos, Kubernetes).\n",
    "\n",
    "Подробнее **[здесь](https://www.oreilly.com/library/view/learning-spark/9781449359034/ch01.html)**.\n",
    "\n",
    "#### Источники данных\n",
    "![Image](images/datasources.png)\n",
    "\n",
    "В теории можем читать-писать в большое кол-во источников и приёмников данных.\n",
    "\n",
    "На практике:\n",
    "* Есть проблемы при взаимодействии с Hive (подробнее будет на лекции),\n",
    "* И при подключении к Cassandra.\n",
    "* Хорошо взаимодействует с Kafka."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Архитектура Spark-приложения\n",
    "![Image](images/cluster-overview.png)\n",
    "(https://spark.apache.org/docs/latest/cluster-overview.html)\n",
    "\n",
    "1. Driver program - управляющая программа.\n",
    "2. SparkContext - это основной объект, с помощью которого мы взаимодействуем со Spark.\n",
    "3. Cluster manager - планировщик (любой, см. выше).\n",
    "4. Executor - по сути JVM на нодах.\n",
    "\n",
    "В 1-м приближении работает также как и Hadoop. Единственное, контейнеры **долго живущие**. Контейнеры поднимаются 1 раз и умирают когда заканчивается SparkContext. Это позволяет хранить данные **в памяти JVM**. Быстрее RAM только кеши CPU, но это сложно реализуется (ассемблер)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Возможности работы со Spark\n",
    "##### Интерактивный shell\n",
    "\n",
    "1. `spark2-shell` - запускает Scala-оболочку.\n",
    "2. `pyspark2` - python оболочку.\n",
    "\n",
    "В этих оболочках уже имеется готовый SparkContext (переменная `sc`).\n",
    "\n",
    "##### Запуск файла на исполнение\n",
    "`spark2-submit [params] <file>` - можем запускать как jar-файлы, так и коды на Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Запуск Spark в Jupyter-ноутбуке:\n",
    "\n",
    "```bash\n",
    "PYSPARK_DRIVER_PYTHON=jupyter PYSPARK_PYTHON=/usr/bin/python3 PYSPARK_DRIVER_PYTHON_OPTS='notebook --ip=\"*\" --port=<PORT> --NotebookApp.token=\"<TOKEN>\" --no-browser' pyspark2 --master=yarn --num-executors=<N>\n",
    "```\n",
    " - **PORT** - порт, на котором откроется ноутбук.\n",
    " - **TOKEN** - токен, который нужно будет ввести для входа в Jupyter (любая строка). Не оставляйте токен пустым т.к. в этом случае к вашему ноутбуку смогут подключаться другие пользователи. `--NotebookApp.token=\"<TOKEN>\"` можно не писать, тогда он сгенерится сам, а посмотреть его можно будет с помощью команды `jupyter notebook list`.\n",
    " - **N** - кол-во executors (YARN containers), выделенных на приложение. \n",
    " \n",
    "Подробнее в [Userguide](https://docs.google.com/document/d/1dmb8o3M2ZCsjPq3rJQqd-jNLQhiBXWbWZcTn9aYUAp8/edit).\n",
    " \n",
    "#### Режимы запуска Spark\n",
    "1. **local**. И драйвер, и worker стартуют на 1 машине. Можно указывать число ядер, выделенных на задачу. Например, `local[3]`. Указывать меньше 2 не рекомендуется т.к. всегда запускает 2 процесса: driver, worker.\n",
    "2. **yarn**. Распределённый режим. Здесь можно дополнительно указать `--deploy-mode`. \n",
    "   * `cluster`. Драйвер на мастере либо на ноде. Рекомендуется для прода.\n",
    "   * `client`. Драйвер на клиенте. Проще отлаживаться. Проще работать в интерактивном режиме (сейчас мы работаем в режиме `client`). Но грузит клиент. \n",
    " \n",
    "В аргументах PySpark можно указывать и многе другое, подробнее [здесь](http://spark.apache.org/docs/latest/configuration.html#application-properties)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://mipt-client.atp-fivt.org:4043\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0.cloudera2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=PySparkShell>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можем изменить конфигурацию SparkContext, правда его придётся перезапустить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "conf = sc.getConf().setAppName(\"the {}\\'s spark app\".format(getpass.getuser())).set(\"spark.python.profile\",\"true\")\n",
    "sc.stop()\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.textFile(\"/data/griboedov\").map(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2681"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile(\"/data/griboedov\").map(lambda x: x.strip()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2681"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Profile of RDD<id=4>\n",
      "============================================================\n",
      "         29590 function calls (29584 primitive calls) in 0.028 seconds\n",
      "\n",
      "   Ordered by: internal time, cumulative time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "     5365    0.020    0.000    0.020    0.000 {method 'read' of '_io.BufferedReader' objects}\n",
      "     2684    0.002    0.000    0.024    0.000 serializers.py:677(loads)\n",
      "     2684    0.001    0.000    0.027    0.000 rdd.py:1055(<genexpr>)\n",
      "     2681    0.001    0.000    0.001    0.000 {method 'decode' of 'bytes' objects}\n",
      "     2684    0.001    0.000    0.002    0.000 serializers.py:714(read_int)\n",
      "     2684    0.001    0.000    0.025    0.000 serializers.py:686(load_stream)\n",
      "     2681    0.001    0.000    0.001    0.000 util.py:97(wrapper)\n",
      "     2681    0.000    0.000    0.001    0.000 <ipython-input-6-a3d81ddb872f>:1(<lambda>)\n",
      "     2684    0.000    0.000    0.000    0.000 {built-in method unpack}\n",
      "        6    0.000    0.000    0.027    0.005 {built-in method sum}\n",
      "        3    0.000    0.000    0.000    0.000 serializers.py:389(dump_stream)\n",
      "     2681    0.000    0.000    0.000    0.000 {method 'strip' of 'str' objects}\n",
      "        3    0.000    0.000    0.028    0.009 worker.py:370(process)\n",
      "      9/3    0.000    0.000    0.028    0.009 rdd.py:2498(pipeline_func)\n",
      "        3    0.000    0.000    0.000    0.000 serializers.py:721(write_int)\n",
      "        9    0.000    0.000    0.028    0.003 rdd.py:351(func)\n",
      "        3    0.000    0.000    0.027    0.009 rdd.py:1055(<lambda>)\n",
      "        3    0.000    0.000    0.000    0.000 serializers.py:575(dumps)\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method dumps}\n",
      "        3    0.000    0.000    0.000    0.000 rdd.py:323(func)\n",
      "        3    0.000    0.000    0.000    0.000 rdd.py:1046(<lambda>)\n",
      "        3    0.000    0.000    0.000    0.000 util.py:92(fail_on_stopiteration)\n",
      "        6    0.000    0.000    0.000    0.000 {method 'write' of '_io.BufferedWriter' objects}\n",
      "        6    0.000    0.000    0.000    0.000 rdd.py:909(func)\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method pack}\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method add}\n",
      "        6    0.000    0.000    0.000    0.000 {built-in method len}\n",
      "        3    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method iter}\n",
      "\n",
      "\n",
      "============================================================\n",
      "Profile of RDD<id=5>\n",
      "============================================================\n",
      "         29590 function calls (29584 primitive calls) in 0.048 seconds\n",
      "\n",
      "   Ordered by: internal time, cumulative time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "     5365    0.035    0.000    0.035    0.000 {method 'read' of '_io.BufferedReader' objects}\n",
      "     2684    0.003    0.000    0.042    0.000 serializers.py:677(loads)\n",
      "     2684    0.002    0.000    0.047    0.000 rdd.py:1055(<genexpr>)\n",
      "     2681    0.002    0.000    0.002    0.000 {method 'decode' of 'bytes' objects}\n",
      "     2684    0.002    0.000    0.003    0.000 serializers.py:714(read_int)\n",
      "     2681    0.001    0.000    0.002    0.000 util.py:97(wrapper)\n",
      "     2684    0.001    0.000    0.043    0.000 serializers.py:686(load_stream)\n",
      "     2681    0.001    0.000    0.001    0.000 <ipython-input-5-60f555e85c0e>:1(<lambda>)\n",
      "     2684    0.000    0.000    0.000    0.000 {built-in method unpack}\n",
      "        6    0.000    0.000    0.047    0.008 {built-in method sum}\n",
      "     2681    0.000    0.000    0.000    0.000 {method 'strip' of 'str' objects}\n",
      "        3    0.000    0.000    0.000    0.000 serializers.py:389(dump_stream)\n",
      "      9/3    0.000    0.000    0.048    0.016 rdd.py:2498(pipeline_func)\n",
      "        3    0.000    0.000    0.048    0.016 worker.py:370(process)\n",
      "        9    0.000    0.000    0.048    0.005 rdd.py:351(func)\n",
      "        3    0.000    0.000    0.000    0.000 serializers.py:721(write_int)\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method dumps}\n",
      "        3    0.000    0.000    0.048    0.016 rdd.py:1055(<lambda>)\n",
      "        3    0.000    0.000    0.000    0.000 serializers.py:575(dumps)\n",
      "        6    0.000    0.000    0.000    0.000 rdd.py:909(func)\n",
      "        3    0.000    0.000    0.000    0.000 rdd.py:323(func)\n",
      "        3    0.000    0.000    0.000    0.000 rdd.py:1046(<lambda>)\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method pack}\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method add}\n",
      "        6    0.000    0.000    0.000    0.000 {method 'write' of '_io.BufferedWriter' objects}\n",
      "        3    0.000    0.000    0.000    0.000 util.py:92(fail_on_stopiteration)\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method iter}\n",
      "        6    0.000    0.000    0.000    0.000 {built-in method len}\n",
      "        3    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sc.show_profiles()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resilient Distributed Dataset и ленивые вычисления\n",
    "\n",
    "RDD - набор данных, распределённый по партициям (аналог сплитов в Hadoop). Основной примитив работы в Spark. \n",
    "\n",
    "##### Свойства\n",
    "* Неизменяемый. Можем получить либо новый RDD, либо plain object\n",
    "* Итерируемый. Можем делать обход RDD\n",
    "* Восстанавливаемый. Каждая партиция помнит как она была получена (часть графа вычислений) и при утере может быть восстановлена."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создать RDD можно:\n",
    "* прочитав данные из источника\n",
    "* получить новый RDD из существующего."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====\t�������������� 1\r\n",
      "����������������\t��������������!.. ����! ������ ���������� �������� ������������!\r\n",
      "����������������\t���������� ������������������ ���������� - ����������,\r\n",
      "����������������\t\"�������� ����������\". - ���������� �������� ���� ��������,\r\n",
      "����������������\t���� ������, ���������������� ���� ������������������ ���� ����������.\r\n",
      "����������������\t������������ ������ ������������ ������ ��������������������,\r\n",
      "����������������\t���� ��������!.. �������������� ����...\r\n",
      "����������������\t��������������,\r\n",
      "����������������\t����! ���������� ����������������, ��������.\r\n",
      "����������������\t���������� ������������ �������� ���� ��������;\r\n",
      "cat: Unable to write to output stream.\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat /data/griboedov/gore_ot_uma-1.txt | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.textFile(\"/data/griboedov\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/data/griboedov MapPartitionsRDD[7] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идём в [SparkHistory UI](http://localhost:18089/) (для этого нужно пробросить порт 18089). Далее переходим в incompleted applications (приложение не завершилось т.к. SparkContext жив) и видим, что в списке Job пусто.\n",
    "\n",
    "Несмотря на это, сам RDD есть:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[8] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем кол-во объектов в RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2681"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Снова проверяем UI и... job'а появилась!\n",
    "\n",
    "В Spark'е сть 2 типа операций над RDD:\n",
    "* [трансформации](https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations). Преобразуют RDD в новое RDD.\n",
    "* [действия](https://spark.apache.org/docs/latest/rdd-programming-guide.html#actions). Преобразуют RDD в обычный объект.\n",
    "\n",
    "Трансформации выполяются **лениво**. При вызове трансформации достраивается граф вычислений и больше ничего не происходит. \n",
    "\n",
    "Реальное выполнение графа происходит при вызове Action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordCount на Spark\n",
    "\n",
    "Мы уже прочитали данные, теперь попробуем посчитать на них WordCount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# строим граф вычислений\n",
    "rdd = sc.textFile(\"/data/griboedov\")\n",
    "rdd = rdd.map(lambda x: x.strip().lower()) # приводим к нижнему регистру\n",
    "rdd = rdd.flatMap(lambda x: x.split(\" \")) # выделяем слова\n",
    "rdd = rdd.map(lambda x: (x, 1))  # собираем пары (word, 1)\n",
    "rdd = rdd.reduceByKey(lambda a, b: a + b) # суммируем \"1\" с одинаковыми ключами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = rdd.sortBy(lambda a: -a[1]) # сортируем по кол-ву встречаемости"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 432),\n",
       " ('в', 344),\n",
       " ('-', 296),\n",
       " ('и', 295),\n",
       " ('не', 287),\n",
       " ('я', 139),\n",
       " ('с', 129),\n",
       " ('на', 126),\n",
       " ('что', 104),\n",
       " ('*', 94)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(10) # Action!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Типы трансформаций в Spark\n",
    "\n",
    "![Image](images/stages.png)\n",
    "(https://www.slideshare.net/LisaHua/spark-overview-37479609)\n",
    "\n",
    "* Часть трансформаций (map, flatmap, ...) обрабатывает партиции независимо. Такие трансформации называются *narrow*. \n",
    "* reduce, sortBy аггрегируют данные и используют передачу по сети. Они называются *wide*. \n",
    "   * Wide-трансформации могут менять кол-во партиций.\n",
    "   * По wide-трансформациям происходит деление job'ы на Stages.\n",
    "\n",
    "Stage тоже делится на task'и. 1 task выполняется для одной партиции.\n",
    "\n",
    "**Итак: Task << Stage << Job << Application.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В Spark есть возможность вывести план job'ы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3) PythonRDD[58] at RDD at PythonRDD.scala:53 []\n",
      " |  MapPartitionsRDD[53] at mapPartitions at PythonRDD.scala:133 []\n",
      " |  ShuffledRDD[52] at partitionBy at NativeMethodAccessorImpl.java:0 []\n",
      " +-(3) PairwiseRDD[51] at sortBy at <ipython-input-20-d4e0638907ea>:1 []\n",
      "    |  PythonRDD[50] at sortBy at <ipython-input-20-d4e0638907ea>:1 []\n",
      "    |  MapPartitionsRDD[47] at mapPartitions at PythonRDD.scala:133 []\n",
      "    |  ShuffledRDD[46] at partitionBy at NativeMethodAccessorImpl.java:0 []\n",
      "    +-(3) PairwiseRDD[45] at reduceByKey at <ipython-input-19-83ffad06e866>:6 []\n",
      "       |  PythonRDD[44] at reduceByKey at <ipython-input-19-83ffad06e866>:6 []\n",
      "       |  /data/griboedov MapPartitionsRDD[43] at textFile at NativeMethodAccessorImpl.java:0 []\n",
      "       |  /data/griboedov HadoopRDD[42] at textFile at NativeMethodAccessorImpl.java:0 []\n"
     ]
    }
   ],
   "source": [
    "print(rdd.toDebugString().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим всего 3 трансформации. Где все остальные?\n",
    "\n",
    "Spark написан на Scala, которая под капотом использует JVM. Чтоб делать вычисления в Python, нужно вытаскивать данные из JVM. А потом возвращаться обратно. Получаем OverHead на сериализацию-десериализацию. Чтоб overhead'ов было меньше, схлопываем узкие трансформации в одну."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Весь пример целиком:** `/home/velkerr/seminars/pd2018/14-15-spark/griboedov.py`\n",
    "\n",
    "Запустим с помощью `spark2-submit griboedov.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 1.\n",
    "\n",
    "> При подсчёте отсеять пунктуацию и слова короче 3 символов. \n",
    "При фильтрации можно использовать регулярку: `re.sub(u\"\\\\W+\", \" \", x.strip(), flags=re.U)`.\n",
    "\n",
    "### Задача 2.\n",
    "\n",
    "> Считать только имена собственные. Именами собственными в данном случае будем считать такие слова, у которых 1-я буква заглавная, остальные - прописные.\n",
    "\n",
    "**Решение**: ` /home/velkerr/seminars/pd2018/14-15-spark/griboedov_adv.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Аккумуляторы\n",
    "\n",
    "Аналоги счётчиков в Hadoop. \n",
    "* Используется для легковесной аггрегации (без `reduceByKey` и дополнительных shuffle'ов)\n",
    "* Если аккумулятор используется в трансформациях, то нельзя гарантировать консистентность (мы можем с помощью action'a вызвать DAG несколько раз). Можно использовать в `foreach()`.\n",
    "\n",
    "**Объявление:** `cnt = sc.accumulator(start_val)`\n",
    "\n",
    "**Использование:** \n",
    "   * Inline: `foreach(lambda x: cnt.add(x))`\n",
    "   * Или же, с помощью своей функции:\n",
    "    ```python\n",
    "    def count_with_conditions(x):\n",
    "        global cnt\n",
    "        if ...:\n",
    "            cnt += 1\n",
    "\n",
    "    rdd.foreach(lambda x: count_with_conditions(x))\n",
    "    ```\n",
    "\n",
    "**Получение результата:** `cnt.value`\n",
    "\n",
    "Подробнее в [документации](http://spark.apache.org/docs/latest/rdd-programming-guide.html#accumulators)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 3.\n",
    "\n",
    "> Переделайте задача 2 так, чтоб кол-во имён собственных вычислялось с помощью аккумулятора.\n",
    "\n",
    "**Решение**: ` /home/velkerr/seminars/pd2018/14-15-spark/griboedov_accum.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcast-переменные\n",
    "\n",
    "Аналог DistributedCache в Hadoop. Обычно используется когда мы хотим в спарке сделать Map-side join (т.е. имеется 2 датасета: 1 маленький, который и добавляем в broadcast, другой большой)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "br_cast = sc.broadcast([\"hadoop\", \"hive\", \"spark\", 'zookeeper', 'kafka']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "br_cast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "br_cast.value[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Кеширование\n",
    "\n",
    "При перезапуске Action, пересчитывается весь граф вычислений. Это логично т.к. в трансформациях ничего не вычисляется. Полезно это тем, что если за время работы задачи данные обновились (дополнились), нам достаточно просто перевызвать Action.\n",
    "\n",
    "Но если данные не меняются (например, при отладке), такой пересчёт даёт Overhead. Можно **закешировать** часть pipeline. Тогда при след. вызове Action, RDD считается с кеша и пересчёт начнётся с того места, где было кеширование. В History UI все Stage перед этим будут помечены \"Skipped\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.textFile(\"/data/griboedov\")\n",
    "rdd = rdd.map(lambda x: x.strip().lower())\n",
    "rdd = rdd.flatMap(lambda x: x.split(\" \"))\n",
    "rdd = rdd.map(lambda x: (x, 1)).cache() # тут всё хорошо работает, кешируем\n",
    "rdd = rdd.reduceByKey(lambda a, b: a + b) # а тут хотим отладить, поэтому будут перезапуски\n",
    "rdd = rdd.sortBy(lambda a: -a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 432), ('в', 344), ('-', 296), ('и', 295), ('не', 287)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[556] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кешировать можно с помощью двух операций:\n",
    "* `cache()`\n",
    "* `persist(storage_level)`\n",
    "\n",
    "В `persist()` можно указать [StorageLevel](https://spark.apache.org/docs/2.1.2/api/python/_modules/pyspark/storagelevel.html), т.е. на какой носитель кешируем. Можем закешировать в диск, в память, на диск и / или память на несколько нод... или дать возможность Spark'у решить самому (на основе объёма кеша).\n",
    "\n",
    "`cache()` - это простой вариант `persist()`, когда кешируем только в RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.textFile(\"/data/griboedov\")\n",
    "rdd = rdd.map(lambda x: x.strip().lower())\n",
    "rdd = rdd.flatMap(lambda x: x.split(\" \"))\n",
    "rdd = rdd.map(lambda x: (x, 1)).cache() # тут всё хорошо работает, кешируем\n",
    "rdd = rdd.reduceByKey(lambda a, b: a + b) # а тут хотим отладить, поэтому будут перезапуски"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = rdd.sortBy(lambda a: -a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[97] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 432), ('в', 344), ('-', 296), ('и', 295), ('не', 287)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Практические задания"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В hdfs в папке `/data/access_logs/big_log` лежит лог в формате\n",
    "\n",
    "* IP-адрес пользователя (`195.206.123.39`),\n",
    "* Далее идут два неиспользуемых в нашем случае поля (`-` и `-`),\n",
    "* Время запроса (`[24/Sep/2015:12:32:53 +0400]`),\n",
    "* Строка запроса (`\"GET /id18222 HTTP/1.1\"`),\n",
    "* HTTP-код ответа (`200`),\n",
    "* Размер ответа (`10703`),\n",
    "* Реферер (источник перехода; `\"http://bing.com/\"`),\n",
    "* Идентификационная строка браузера (User-Agent; `\"Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/37.0.2062.94 Safari/537.36\"`).\n",
    "\n",
    "Созданы несколько семплов данных разного размера:\n",
    "```\n",
    "3.4 G    10.2 G   /data/access_logs/big_log\n",
    "17.6 M   52.7 M   /data/access_logs/big_log_10000\n",
    "175.4 M  526.2 M  /data/access_logs/big_log_100000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример парсинга логов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATASET = \"/data/access_logs/big_log_10000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime as dt\n",
    "\n",
    "log_format = re.compile( \n",
    "    r\"(?P<host>[\\d\\.]+)\\s\" \n",
    "    r\"(?P<identity>\\S*)\\s\" \n",
    "    r\"(?P<user>\\S*)\\s\"\n",
    "    r\"\\[(?P<time>.*?)\\]\\s\"\n",
    "    r'\"(?P<request>.*?)\"\\s'\n",
    "    r\"(?P<status>\\d+)\\s\"\n",
    "    r\"(?P<bytes>\\S*)\\s\"\n",
    "    r'\"(?P<referer>.*?)\"\\s'\n",
    "    r'\"(?P<user_agent>.*?)\"\\s*'\n",
    ")\n",
    "\n",
    "def parseLine(line):\n",
    "    match = log_format.match(line)\n",
    "    if not match:\n",
    "        return (\"\", \"\", \"\", \"\", \"\", \"\", \"\" ,\"\", \"\")\n",
    "\n",
    "    request = match.group('request').split()\n",
    "    return (match.group('host'), match.group('time').split()[0], \\\n",
    "       request[0], request[1], match.group('status'), match.group('bytes'), \\\n",
    "        match.group('referer'), match.group('user_agent'),\n",
    "        dt.strptime(match.group('time').split()[0], '%d/%b/%Y:%H:%M:%S').hour)\n",
    "\n",
    "\n",
    "lines = sc.textFile(DATASET)\n",
    "parsed_logs = lines.map(parseLine).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('109.105.128.100',\n",
       "  '10/Dec/2015:00:00:00',\n",
       "  'GET',\n",
       "  '/id45574',\n",
       "  '200',\n",
       "  '27513',\n",
       "  '-',\n",
       "  'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.99 Safari/537.36',\n",
       "  0),\n",
       " ('217.146.45.122',\n",
       "  '10/Dec/2015:00:00:00',\n",
       "  'GET',\n",
       "  '/id40851',\n",
       "  '200',\n",
       "  '11914',\n",
       "  '-',\n",
       "  'Mozilla/5.0 (X11; Linux i686; rv:10.0.4) Gecko/20120421 Firefox/10.0.4',\n",
       "  0),\n",
       " ('17.72.78.198',\n",
       "  '10/Dec/2015:00:00:00',\n",
       "  'GET',\n",
       "  '/id58931',\n",
       "  '200',\n",
       "  '32457',\n",
       "  '-',\n",
       "  'Mozilla/5.0; TOB 6.11 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko',\n",
       "  0),\n",
       " ('46.245.183.68',\n",
       "  '10/Dec/2015:00:00:00',\n",
       "  'GET',\n",
       "  '/id19513',\n",
       "  '200',\n",
       "  '26190',\n",
       "  '-',\n",
       "  'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.99 Safari/537.36',\n",
       "  0),\n",
       " ('91.197.164.156',\n",
       "  '10/Dec/2015:00:00:01',\n",
       "  'GET',\n",
       "  '/id39028',\n",
       "  '200',\n",
       "  '14115',\n",
       "  '-',\n",
       "  'Mozilla/5.0 (X11; Fedora; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.125 Safari/537.36',\n",
       "  0),\n",
       " ('93.92.113.28',\n",
       "  '10/Dec/2015:00:00:01',\n",
       "  'GET',\n",
       "  '/id62078',\n",
       "  '200',\n",
       "  '18177',\n",
       "  '-',\n",
       "  'Mozilla/5.0 (iPad; CPU OS 9_0 like Mac OS X) AppleWebKit/600.1.4 (KHTML, like Gecko) CriOS/45.0.2454.89 Mobile/13A344 Safari/600.1.4',\n",
       "  0),\n",
       " ('91.238.133.210',\n",
       "  '10/Dec/2015:00:00:01',\n",
       "  'GET',\n",
       "  '/id92791',\n",
       "  '200',\n",
       "  '19835',\n",
       "  '-',\n",
       "  'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.99 Safari/537.36',\n",
       "  0),\n",
       " ('93.92.113.28',\n",
       "  '10/Dec/2015:00:00:02',\n",
       "  'GET',\n",
       "  '/id67125',\n",
       "  '200',\n",
       "  '23795',\n",
       "  '-',\n",
       "  'Mozilla/5.0 (iPad; CPU OS 9_0 like Mac OS X) AppleWebKit/600.1.4 (KHTML, like Gecko) CriOS/45.0.2454.89 Mobile/13A344 Safari/600.1.4',\n",
       "  0),\n",
       " ('92.227.170.8',\n",
       "  '10/Dec/2015:00:00:02',\n",
       "  'GET',\n",
       "  '/id43630',\n",
       "  '200',\n",
       "  '9841',\n",
       "  '-',\n",
       "  'Mozilla/5.0 (Linux; Android 5.0.1; SCH-I545 Build/LRX22C) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/43.0.2357.93 Mobile Safari/537.36',\n",
       "  0),\n",
       " ('185.36.12.188',\n",
       "  '10/Dec/2015:00:00:03',\n",
       "  'GET',\n",
       "  '/id34798',\n",
       "  '200',\n",
       "  '15053',\n",
       "  '-',\n",
       "  'Mozilla/5.0 (Windows NT 5.1; U; de; rv:1.9.1.6) Gecko/20091201 Firefox/3.5.6 Opera 11.00',\n",
       "  0)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_logs.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2й вариант парсинга - с помощью namedtuple\n",
    "\n",
    "```python\n",
    "LogItem = namedtuple(\"LogItem\", \n",
    "                     [\"host\", \"time\", \"method\", \"path\", \"status\", \"length\", \"referer\", \"user_agent\", \"hour\"])\n",
    "\n",
    "def parseLine(line):\n",
    "    match = log_format.match(line)\n",
    "    if not match:\n",
    "        return LogItem(\"\", \"\", \"\", \"\", \"\", \"\", \"\" ,\"\", \"\")\n",
    "\n",
    "    request = match.group('request').split()\n",
    "    return LogItem(\n",
    "        host=match.group('host'),\n",
    "        time=match.group('time').split()[0],\n",
    "        method=request[0],\n",
    "        path=request[1],\n",
    "        status=match.group('status'),\n",
    "        length=match.group('bytes'),\n",
    "        referer=match.group('referer'),\n",
    "        user_agent=match.group('user_agent'),\n",
    "        hour=dt.strptime(match.group('time').split()[0],'%d/%b/%Y:%H:%M:%S').hour\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Распарсили, получили RDD, закешировали."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('109.105.128.100',\n",
       "  '10/Dec/2015:00:00:00',\n",
       "  'GET',\n",
       "  '/id45574',\n",
       "  '200',\n",
       "  '27513',\n",
       "  '-',\n",
       "  'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.99 Safari/537.36',\n",
       "  0)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_logs.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "from pyspark.sql import SparkSession, Row\n",
    "\n",
    "log_format = re.compile(\n",
    "    r\"(?P<host>[\\d\\.]+)\\s\"\n",
    "    r\"(?P<identity>\\S*)\\s\"\n",
    "    r\"(?P<user>\\S*)\\s\"\n",
    "    r\"\\[(?P<time>.*?)\\]\\s\"\n",
    "    r'\"(?P<request>.*?)\"\\s'\n",
    "    r\"(?P<status>\\d+)\\s\"\n",
    "    r\"(?P<bytes>\\S*)\\s\"\n",
    "    r'\"(?P<referer>.*?)\"\\s'\n",
    "    r'\"(?P<user_agent>.*?)\"\\s*'\n",
    ")\n",
    "\n",
    "\n",
    "def parseLine(line):\n",
    "    match = log_format.match(line)\n",
    "    if not match:\n",
    "        return (\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\")\n",
    "\n",
    "    request = match.group('request').split()\n",
    "    return (match.group('host'), match.group('time').split()[0],\n",
    "        request[0], request[1], match.group('status'), int(match.group('bytes')),\n",
    "        match.group('referer'), match.group('user_agent'),\n",
    "        dt.strptime(match.group('time').split()[0], '%d/%b/%Y:%H:%M:%S').hour)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spark_session = SparkSession.builder.master(\"yarn\").appName(\"501 df\").config(\"spark.ui.port\", \"18089\").getOrCreate()\n",
    "    lines = spark_session.sparkContext.textFile(\"%s\" % sys.argv[1])\n",
    "    parts = lines.map(parseLine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 4.\n",
    "> Напишите программу, выводящую на экран TOP5 ip адресов, в которых содержится хотя бы одна цифра 4, с наибольшим количеством посещений.\n",
    "Каждая строка результата должна содержать IP адрес и число посещений, разделенные табуляцией, строки должны быть упорядочены по числу посещений по убыванию, например:\n",
    "```\n",
    "195.206.123.39<TAB>40\n",
    "196.206.123.40<TAB>39\n",
    "191.206.123.41<TAB>38\n",
    "175.206.123.42<TAB>37\n",
    "195.236.123.43<TAB>36\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[82] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 5.\n",
    ">  Напишите программу, выводящую на экран суммарное распределение количества посетителей по часам (для каждого часа в сутках вывести количество посетителей, пришедших в этот час). Id посетителя = ip + user_agent.\n",
    "Результат должен содержать час в сутках и число посетителей, разделенные табом и упорядоченные по часам. Например:\n",
    "```\n",
    "0<tab>10\n",
    "1<tab>10\n",
    "2<tab>10\n",
    "…..\n",
    "21<tab>30\n",
    "22<tab>20\n",
    "23<tab>10\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[82] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('csv').option('sep', ' ').load('/data/access_logs/big_log_10000')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_c0</th>\n",
       "      <th>_c1</th>\n",
       "      <th>_c2</th>\n",
       "      <th>_c3</th>\n",
       "      <th>_c4</th>\n",
       "      <th>_c5</th>\n",
       "      <th>_c6</th>\n",
       "      <th>_c7</th>\n",
       "      <th>_c8</th>\n",
       "      <th>_c9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>109.106.133.8</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>[12/Dec/2015:01:31:46</td>\n",
       "      <td>+0400]</td>\n",
       "      <td>GET /id53821 HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>21546</td>\n",
       "      <td>-</td>\n",
       "      <td>Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4)...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             _c0 _c1 _c2                    _c3     _c4  \\\n",
       "0  109.106.133.8   -   -  [12/Dec/2015:01:31:46  +0400]   \n",
       "\n",
       "                     _c5  _c6    _c7 _c8  \\\n",
       "0  GET /id53821 HTTP/1.1  200  21546   -   \n",
       "\n",
       "                                                 _c9  \n",
       "0  Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4)...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.limit(1).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv('/data/twitter/twitter_sample_small.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_c0</th>\n",
       "      <th>_c1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>2241</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  _c0   _c1\n",
       "0  12  2241"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.limit(1).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+\n",
      "|destination|source|\n",
      "+-----------+------+\n",
      "|         12|  2241|\n",
      "+-----------+------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    F.col(\"_c0\").alias(\"destination\"),\n",
    "    F.col(\"_c1\").alias(\"source\")\n",
    ").show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1 = df.select(\n",
    "    F.col(\"_c0\").alias(\"destination\"),\n",
    "    F.col(\"_c1\").alias(\"source\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>destination</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>2241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>13349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>41873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>82473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>414853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>755452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>12</td>\n",
       "      <td>758983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>12</td>\n",
       "      <td>793023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>12</td>\n",
       "      <td>794748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12</td>\n",
       "      <td>806280</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  destination  source\n",
       "0          12    2241\n",
       "1          12   13349\n",
       "2          12   41873\n",
       "3          12   82473\n",
       "4          12  414853\n",
       "5          12  755452\n",
       "6          12  758983\n",
       "7          12  793023\n",
       "8          12  794748\n",
       "9          12  806280"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|  source|count|\n",
      "+--------+-----+\n",
      "|19593065|    1|\n",
      "|20651832|    1|\n",
      "|21215059|    1|\n",
      "|21240863|    1|\n",
      "|21705463|    1|\n",
      "|22197844|    1|\n",
      "|22377590|    1|\n",
      "|22935113|    1|\n",
      "|23993819|    1|\n",
      "|24268091|    1|\n",
      "|24299946|    1|\n",
      "|24326074|    1|\n",
      "|25053014|    1|\n",
      "|26715269|    1|\n",
      "|27121291|    1|\n",
      "|27385940|    1|\n",
      "|27628353|    1|\n",
      "|28817312|    1|\n",
      "|29847995|    1|\n",
      "|30290044|    1|\n",
      "+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.groupBy('source').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|  source|count|\n",
      "+--------+-----+\n",
      "|      53|    7|\n",
      "| 9598762|    4|\n",
      "|15458708|    4|\n",
      "|13342022|    4|\n",
      "|      20|    4|\n",
      "|19489341|    4|\n",
      "|      12|    4|\n",
      "|14206015|    3|\n",
      "|26468557|    3|\n",
      "|14287820|    3|\n",
      "|     107|    3|\n",
      "|52041136|    3|\n",
      "|17184081|    3|\n",
      "|19788155|    3|\n",
      "|21494147|    3|\n",
      "|18662758|    3|\n",
      "|18234522|    3|\n",
      "|16227030|    3|\n",
      "|47516482|    3|\n",
      "|      23|    3|\n",
      "+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.groupBy('source').count().orderBy(F.col('count').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+-----------+\n",
      "|  source|count|       mean|\n",
      "+--------+-----+-----------+\n",
      "|19593065|    1|1.9593065E7|\n",
      "|20651832|    1|2.0651832E7|\n",
      "|21215059|    1|2.1215059E7|\n",
      "|21240863|    1|2.1240863E7|\n",
      "|21705463|    1|2.1705463E7|\n",
      "|22197844|    1|2.2197844E7|\n",
      "|22377590|    1| 2.237759E7|\n",
      "|22935113|    1|2.2935113E7|\n",
      "|23993819|    1|2.3993819E7|\n",
      "|24268091|    1|2.4268091E7|\n",
      "+--------+-----+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum,avg,max,count\n",
    "\n",
    "df1.groupBy(\"source\").agg(count(\"*\").alias(\"count\"), avg(\"source\").alias(\"mean\")).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12,422,53,52,107,20,23,274,34\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "TweeterDF = spark.read.format(\"csv\")\\\n",
    "          .option(\"sep\", \"\\t\")\\\n",
    "          .load(\"/data/twitter/twitter_sample_small.txt\")\n",
    "\n",
    "PeakDF = TweeterDF\\\n",
    "    .select(\n",
    "        F.col(\"_c0\").alias(\"destination\"),\n",
    "        F.col(\"_c1\").alias(\"source\"))\\\n",
    "    .groupBy(\"source\")\\\n",
    "    .agg(F.collect_list(F.col(\"destination\")).alias(\"all_peak\"))\\\n",
    "    .persist()\n",
    "\n",
    "NewStageDF = PeakDF\\\n",
    "    .filter(F.col(\"source\") == F.lit(\"12\"))\\\n",
    "    .select(\n",
    "        F.col(\"source\").alias(\"path\"),\n",
    "        F.col(\"all_peak\").alias(\"last_peak\"))\n",
    "\n",
    "while not NewStageDF.select(F.expr(\"array_contains(last_peak, '34')\")).first()[0]:\n",
    "    NewStageDF = NewStageDF\\\n",
    "        .alias(\"init\")\\\n",
    "        .join(\n",
    "            PeakDF.alias(\"df\"),\n",
    "            F.expr(\"array_contains(init.last_peak, df.source)\"))\\\n",
    "        .distinct()\\\n",
    "        .select(\n",
    "            F.concat_ws(\",\", F.col(\"init.path\"), F.col(\"df.source\")).alias(\"path\"),\n",
    "            F.col(\"df.all_peak\").alias(\"last_peak\"))\n",
    "\n",
    "path = NewStageDF\\\n",
    "    .filter(F.expr(\"array_contains(last_peak, '34')\"))\\\n",
    "    .select(F.concat_ws(\",\", F.col(\"path\"), F.lit(\"34\")).alias(\"path\"))\\\n",
    "    .collect()[0][0]\n",
    "\n",
    "PeakDF.unpersist()\n",
    "\n",
    "print(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
